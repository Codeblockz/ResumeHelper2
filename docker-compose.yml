version: '3.8'

services:
  # Frontend - Next.js Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: development
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /app/.next
    environment:
      - NODE_ENV=development
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    depends_on:
      - backend
    networks:
      - app-network

  # Backend - FastAPI Application
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - resume-uploads:/app/uploads
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/resume_tailor
      - REDIS_URL=redis://redis:6379
      - OLLAMA_URL=http://ollama:11434
      - ENVIRONMENT=development
    depends_on:
      - db
      - redis
      - ollama
    networks:
      - app-network

  # Database - PostgreSQL
  db:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=resume_tailor
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - app-network

  # Cache/Session Store - Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - app-network

  # LLM Service - Ollama
  ollama:
    build:
      context: ./docker/ollama
      dockerfile: Dockerfile
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
    networks:
      - app-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Fallback for non-GPU systems
    profiles:
      - gpu

  # Non-GPU Ollama fallback
  ollama-cpu:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
    networks:
      - app-network
    profiles:
      - cpu

  # Reverse Proxy - Nginx
  nginx:
    build:
      context: ./nginx
      dockerfile: Dockerfile
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - nginx-logs:/var/log/nginx
    depends_on:
      - frontend
      - backend
    networks:
      - app-network

volumes:
  postgres-data:
  redis-data:
  ollama-models:
  resume-uploads:
  nginx-logs:

networks:
  app-network:
    driver: bridge
# Resume Tailor

AI-powered resume tailoring application that uses Ollama (local LLM) to automatically customize resumes for specific job postings. Optimize your resumes for ATS compatibility and increase your interview opportunities.

## ğŸŒŸ Features

- **AI-Powered Tailoring**: Uses local LLM models to customize resumes for specific job descriptions
- **ATS Optimization**: Ensures proper formatting and keyword density for Applicant Tracking Systems
- **Privacy-First**: All processing happens locally - your resume data never leaves your device
- **Multiple Formats**: Support for PDF, DOCX, and plain text resume formats
- **Real-time Preview**: Side-by-side comparison of original vs. tailored resume
- **Multi-Job Management**: Save and manage multiple tailored versions

## ğŸ—ï¸ Architecture

The application uses a microservices architecture with Docker containers:

- **Frontend**: Next.js with Tailwind CSS
- **Backend**: FastAPI (Python) with PostgreSQL
- **LLM Service**: Ollama with local models
- **Proxy**: Nginx for routing and load balancing
- **Cache**: Redis for session management

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- At least 8GB RAM (for LLM models)
- 10GB free disk space

### Setup

1. **Clone and setup the project:**
   ```bash
   git clone <repository-url>
   cd ResumeHelper2
   ./scripts/setup.sh
   ```

2. **Start development environment:**
   ```bash
   ./scripts/dev.sh
   ```

3. **Setup Ollama models:**
   ```bash
   ./scripts/ollama-setup.sh
   ```

4. **Access the application:**
   - Frontend: http://localhost
   - API Documentation: http://localhost:8000/docs
   - API Health: http://localhost:8000/health

## ğŸ”§ Development

### Environment Configuration

Copy `.env.example` to `.env` and update the values:

```bash
cp .env.example .env
```

Key configuration options:
- `OLLAMA_MODEL`: Default LLM model (llama3.1, mistral, codellama)
- `DATABASE_URL`: PostgreSQL connection string
- `SECRET_KEY`: JWT signing key (auto-generated by setup script)

### Development Workflow

#### Script Purposes
- **`./scripts/setup.sh`** - One-time initialization (environment prep, builds images)
- **`./scripts/dev.sh`** - Daily development (starts full containerized environment)
- **`./scripts/dev-local-ollama.sh`** - Daily development (uses your existing local Ollama)
- **`./scripts/ollama-setup.sh`** - Install and manage AI models (for containerized Ollama)
- **Test commands** - Quality assurance (run inside existing containers)

#### Development Options

**Option 1: Use Local Ollama (Recommended if you have Ollama installed)**
- âœ… Faster startup (no Ollama container to build)
- âœ… Use existing models (no re-downloading)  
- âœ… Better performance (direct GPU access)
- âœ… Familiar ollama commands work

```bash
# First time setup
./scripts/setup.sh

# Daily development with local Ollama
./scripts/dev-local-ollama.sh    # Uses your existing Ollama installation
# ... make code changes ...
docker-compose exec backend pytest
docker-compose down
```

**Option 2: Full Containerized Environment**
- âœ… Complete isolation from local system
- âœ… Easier team consistency
- âœ… Self-contained deployment

```bash
# First time setup
./scripts/setup.sh
./scripts/ollama-setup.sh        # Install models in container

# Daily development with containerized Ollama
./scripts/dev.sh                 # Starts all containers including Ollama
# ... make code changes ...
docker-compose exec backend pytest
docker-compose down
```

**Advanced Usage**
```bash
# Manual container control
docker-compose up frontend backend db redis nginx  # Skip Ollama containers
COMPOSE_PROFILES=gpu docker-compose up             # Use GPU acceleration
docker-compose logs -f backend                     # View specific logs
docker-compose down -v --remove-orphans           # Complete cleanup
```

### Project Structure

```
ResumeHelper2/
â”œâ”€â”€ frontend/           # Next.js application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/       # App router pages
â”‚   â”‚   â”œâ”€â”€ components/# React components
â”‚   â”‚   â””â”€â”€ lib/       # Utilities and API clients
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ backend/           # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/       # API routes
â”‚   â”‚   â”œâ”€â”€ core/      # Configuration and database
â”‚   â”‚   â”œâ”€â”€ models/    # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ services/  # Business logic
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ nginx/             # Reverse proxy configuration
â”œâ”€â”€ docker/            # Docker configurations
â””â”€â”€ scripts/           # Development and deployment scripts
```

## ğŸ¤– Ollama Models

Recommended models for resume tailoring:

1. **llama3.1** (4.7GB) - Best overall performance
2. **mistral** (4.1GB) - Fast and efficient  
3. **codellama** (3.8GB) - Good for structured content

Install models using:
```bash
./scripts/ollama-setup.sh
```

Or manually:
```bash
docker exec -it <ollama-container> ollama pull llama3.1
```

## ğŸ“ˆ Production Deployment

1. **Update environment for production:**
   ```bash
   # In .env file
   ENVIRONMENT=production
   DEBUG=false
   SECRET_KEY=your-production-secret-key
   ALLOWED_HOSTS=yourdomain.com
   ```

2. **Deploy:**
   ```bash
   ./scripts/prod.sh
   ```

3. **SSL Configuration:**
   - Place SSL certificates in `nginx/ssl/`
   - Update `nginx/nginx.prod.conf` with your domain

## ğŸ§ª Testing

```bash
# Backend tests
docker-compose exec backend pytest

# Frontend tests  
docker-compose exec frontend npm test

# Integration tests
docker-compose -f docker-compose.test.yml up --abort-on-container-exit
```

## ğŸ”’ Security

- All resume processing happens locally
- No data sent to external services
- JWT-based authentication
- Rate limiting and CORS protection
- Security headers in nginx

## ğŸ“Š Performance

Target performance metrics:
- Resume parsing: < 5 seconds
- Job analysis: < 3 seconds
- AI tailoring: < 30 seconds
- Export generation: < 10 seconds

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests
5. Submit a pull request

## ğŸ“ License

[License information to be added]

## ğŸ†˜ Troubleshooting

### Common Issues

**Ollama not responding:**
```bash
docker-compose logs ollama-cpu
docker-compose restart ollama-cpu
```

**Database connection errors:**
```bash
docker-compose down -v
docker-compose up -d db
docker-compose up
```

**Frontend/Backend not communicating:**
- Check nginx logs: `docker-compose logs nginx`
- Verify network connectivity: `docker-compose exec frontend ping backend`

### Getting Help

- Check logs: `docker-compose logs <service-name>`
- Restart services: `docker-compose restart <service-name>`
- Full reset: `docker-compose down -v && docker-compose up --build`

For more help, please open an issue with:
- Your operating system
- Docker version
- Error logs
- Steps to reproduce